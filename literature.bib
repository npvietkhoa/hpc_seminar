
@incollection{jagode_automatic_2020,
	location = {Cham},
	title = {Automatic Code Motion to Extend {MPI} Nonblocking Overlap Window},
	volume = {12321},
	isbn = {978-3-030-59850-1 978-3-030-59851-8},
	url = {https://link.springer.com/10.1007/978-3-030-59851-8_4},
	abstract = {{HPC} applications rely on a distributed-memory parallel programming model to improve the overall execution time. This leads to spawning multiple processes that need to communicate with each other to make the code progress. But these communications involve overheads caused by network latencies or synchronizations between processes. One possible approach to reduce those overheads is to overlap communications with computations. {MPI} allows this solution through its nonblocking communication mode: a nonblocking communication is composed of an initialization and a completion call. It is then possible to overlap the communication by inserting computations between these two calls. The use of nonblocking collective calls is however still marginal and adds a new layer of complexity. In this paper we propose an automatic static optimization that (i) transforms blocking {MPI} communications into their nonblocking counterparts and (ii) performs extensive code motion to increase the size of overlapping intervals between initialization and completion calls. Our method is implemented in {LLVM} as a compilation pass, and shows promising results on two mini applications.},
	pages = {43--54},
	booktitle = {High Performance Computing},
	publisher = {Springer International Publishing},
	author = {Nguyen, Van Man and Saillard, Emmanuelle and Jaeger, Julien and Barthou, Denis and Carribault, Patrick},
	editor = {Jagode, Heike and Anzt, Hartwig and Juckeland, Guido and Ltaief, Hatem},
	urldate = {2023-03-27},
	date = {2020},
	langid = {english},
	doi = {10.1007/978-3-030-59851-8_4},
	note = {Series Title: Lecture Notes in Computer Science},
	file = {Nguyen et al. - 2020 - Automatic Code Motion to Extend MPI Nonblocking Ov.pdf:/Users/npvietkhoa/Zotero/storage/BV8LQWN6/Nguyen et al. - 2020 - Automatic Code Motion to Extend MPI Nonblocking Ov.pdf:application/pdf},
}

@inproceedings{ahmed_transforming_2017,
	location = {New York, {NY}, {USA}},
	title = {Transforming blocking {MPI} collectives to Non-blocking and persistent operations},
	isbn = {978-1-4503-4849-2},
	url = {https://dl.acm.org/doi/10.1145/3127024.3127033},
	doi = {10.1145/3127024.3127033},
	series = {{EuroMPI} '17},
	abstract = {This paper describes Petal, a prototype tool that uses compiler-analysis techniques to automate code transformations to hide communication costs behind computation by replacing blocking {MPI} functions with corresponding nonblocking and persistent collective operations while maintaining legacy applications' correctness. In earlier work, we have already demonstrated Petal's ability to transform point-to-point {MPI} operations in complement to the results shown here. The contributions of this paper include the approach to collective operation transformations, a description of achieved functionality, examples of transformations, and demonstration of performance improvements obtained thus far on representative sample {MPI} programs. Depending on system scale and problem size, the transformations yield a speedup of up to a factor of two. This tool can be used to transform useful classes of new and legacy {MPI} programs to use the newest variants of {MPI} functions to improve performance without manual intervention for forthcoming {HPC} systems and updated versions of the {MPI} standard.},
	pages = {1--11},
	booktitle = {Proceedings of the 24th European {MPI} Users' Group Meeting},
	publisher = {Association for Computing Machinery},
	author = {Ahmed, Hadia and Skjellumh, Anthony and Bangalore, Purushotham and Pirkelbauer, Peter},
	urldate = {2023-04-24},
	date = {2017-09-25},
	keywords = {blocking /nonblocking operations, collective operations, communication-computation overlap, {MPI}, persistent operations},
	file = {Full Text PDF:/Users/npvietkhoa/Zotero/storage/LCCM4B9H/Ahmed et al. - 2017 - Transforming blocking MPI collectives to Non-block.pdf:application/pdf},
}

@inproceedings{hoefler_implementation_2007,
	location = {New York, {NY}, {USA}},
	title = {Implementation and performance analysis of non-blocking collective operations for {MPI}},
	isbn = {978-1-59593-764-3},
	url = {https://dl.acm.org/doi/10.1145/1362622.1362692},
	doi = {10.1145/1362622.1362692},
	series = {{SC} '07},
	abstract = {Collective operations and non-blocking point-to-point operations have always been part of {MPI}. Although non-blocking collective operations are an obvious extension to {MPI}, there have been no comprehensive studies of this functionality. In this paper we present {LibNBC}, a portable high-performance library for implementing non-blocking collective {MPI} communication operations. {LibNBC} provides non-blocking versions of all {MPI} collective operations, is layered on top of {MPI}-1, and is portable to nearly all parallel architectures. To measure the performance characteristics of our implementation, we also present a microbenchmark for measuring both latency and overlap of computation and communication. Experimental results demonstrate that the blocking performance of the collective operations in our library is comparable to that of collective operations in other high-performance {MPI} implementations. Our library introduces a very low overhead between the application and the underlying {MPI} and thus, in conjunction with the potential to overlap communication with computation, offers the potential for optimizing real-world applications.},
	pages = {1--10},
	booktitle = {Proceedings of the 2007 {ACM}/{IEEE} conference on Supercomputing},
	publisher = {Association for Computing Machinery},
	author = {Hoefler, Torsten and Lumsdaine, Andrew and Rehm, Wolfgang},
	urldate = {2023-04-24},
	date = {2007-11-10},
	keywords = {collective operations, {MPI}, non-blocking collective operations, non-blocking communication, overlap},
	file = {Full Text PDF:/Users/npvietkhoa/Zotero/storage/QC9HYB28/Hoefler et al. - 2007 - Implementation and performance analysis of non-blo.pdf:application/pdf},
}

@inproceedings{preissl_using_2008,
	location = {Berlin, Heidelberg},
	title = {Using {MPI} Communication Patterns to Guide Source Code Transformations},
	isbn = {978-3-540-69389-5},
	doi = {10.1007/978-3-540-69389-5_29},
	series = {Lecture Notes in Computer Science},
	abstract = {Optimizing the performance of {HPC} software requires a high-level understanding of communication patterns as well as their relation to source code structures. We describe an algorithm to detect communication patterns in parallel traces and show how these patterns can guide static code analysis. First, we detect patterns that identify potential bottlenecks in {MPI} communication traces. Next, we associate the patterns with the corresponding nodes in an abstract syntaxtree using the {ROSE} compiler framework. Finally we perform static analysis on the annotated control flow and system dependence graphs to guide transformations such as code motion or the automatic introduction of {MPI} collectives.},
	pages = {253--260},
	booktitle = {Computational Science – {ICCS} 2008},
	publisher = {Springer},
	author = {Preissl, Robert and Schulz, Martin and Kranzlmüller, Dieter and de Supinski, Bronis R. and Quinlan, Daniel J.},
	editor = {Bubak, Marian and van Albada, Geert Dick and Dongarra, Jack and Sloot, Peter M. A.},
	date = {2008},
	langid = {english},
	keywords = {Abstract Syntax Tree, Broadcast Operation, Communication Pattern, Lawrence Livermore National Laboratory, Maximal Repeat},
	file = {Full Text:/Users/npvietkhoa/Zotero/storage/SXX9KPML/Preissl et al. - 2008 - Using MPI Communication Patterns to Guide Source C.pdf:application/pdf},
}

@article{barigou_maximizing_2017,
	title = {Maximizing Communication–Computation Overlap Through Automatic Parallelization and Run-time Tuning of Non-blocking Collective Operations},
	volume = {45},
	issn = {1573-7640},
	url = {https://doi.org/10.1007/s10766-016-0477-7},
	doi = {10.1007/s10766-016-0477-7},
	abstract = {Non-blocking collective communication operations extend the concept of collective operations by offering the additional benefit of being able to overlap communication and computation. They are often considered key building blocks for scaling applications to very large process counts. Yet, using non-blocking collective operations in real-world applications is non-trivial. Application codes often have to be restructured significantly in order to maximize the communication–computation overlap. This paper presents an approach to maximize the communication–computation overlap for hybrid {OpenMP}/{MPI} applications. The work leverages automatic parallelization by extending the ability of an existing tool to utilize non-blocking collective operations. It further integrates run-time auto-tuning techniques of non-blocking collective operations, optimizing both, the algorithms used for the non-blocking collective operations as well as location and frequency of accompanying progress function calls. Four application benchmarks were used to demonstrate the efficiency and versatility of the approach on two different platforms. The results indicate significant performance improvements in virtually all test scenarios. The resulting parallel applications achieved a performance improvement of up to 43\% compared to the version using blocking communication operations, and up to 95\% of the maximum theoretical communication–computation overlap identified for each scenario.},
	pages = {1390--1416},
	number = {6},
	journaltitle = {International Journal of Parallel Programming},
	shortjournal = {Int J Parallel Prog},
	author = {Barigou, Youcef and Gabriel, Edgar},
	urldate = {2023-04-25},
	date = {2017-12-01},
	langid = {english},
	keywords = {{MPI}, Auto-tuning, Communication-computation overlap, Non-blocking collective operations, {OpenMP}},
	file = {Full Text PDF:/Users/npvietkhoa/Zotero/storage/TIRPF7G9/Barigou and Gabriel - 2017 - Maximizing Communication–Computation Overlap Throu.pdf:application/pdf},
}

@inproceedings{gabriel_open_2004,
	location = {Berlin, Heidelberg},
	title = {Open {MPI}: Goals, Concept, and Design of a Next Generation {MPI} Implementation},
	isbn = {978-3-540-30218-6},
	doi = {10.1007/978-3-540-30218-6_19},
	series = {Lecture Notes in Computer Science},
	shorttitle = {Open {MPI}},
	abstract = {A large number of {MPI} implementations are currently available, each of which emphasize different aspects of high-performance computing or are intended to solve a specific research problem. The result is a myriad of incompatible {MPI} implementations, all of which require separate installation, and the combination of which present significant logistical challenges for end users. Building upon prior research, and influenced by experience gained from the code bases of the {LAM}/{MPI}, {LA}-{MPI}, and {FT}-{MPI} projects, Open {MPI} is an all-new, production-quality {MPI}-2 implementation that is fundamentally centered around component concepts. Open {MPI} provides a unique combination of novel features previously unavailable in an open-source, production-quality implementation of {MPI}. Its component architecture provides both a stable platform for third-party research as well as enabling the run-time composition of independent software add-ons. This paper presents a high-level overview the goals, design, and implementation of Open {MPI}.},
	pages = {97--104},
	booktitle = {Recent Advances in Parallel Virtual Machine and Message Passing Interface},
	publisher = {Springer},
	author = {Gabriel, Edgar and Fagg, Graham E. and Bosilca, George and Angskun, Thara and Dongarra, Jack J. and Squyres, Jeffrey M. and Sahay, Vishal and Kambadur, Prabhanjan and Barrett, Brian and Lumsdaine, Andrew and Castain, Ralph H. and Daniel, David J. and Graham, Richard L. and Woodall, Timothy S.},
	editor = {Kranzlmüller, Dieter and Kacsuk, Péter and Dongarra, Jack},
	date = {2004},
	langid = {english},
	keywords = {Collective Operation, Component Architecture, Component Framework, Message Passing Interface, Reference Count},
	file = {Full Text PDF:/Users/npvietkhoa/Zotero/storage/V6WHP2PD/Gabriel et al. - 2004 - Open MPI Goals, Concept, and Design of a Next Gen.pdf:application/pdf},
}

@inproceedings{hoefler_leveraging_2008,
	location = {Munich Germany},
	title = {Leveraging non-blocking collective communication in high-performance applications},
	isbn = {978-1-59593-973-9},
	url = {https://dl.acm.org/doi/10.1145/1378533.1378554},
	doi = {10.1145/1378533.1378554},
	abstract = {Although overlapping communication with computation is an important mechanism for achieving high performance in parallel programs, developing applications that actually achieve good overlap can be difﬁcult. Existing approaches are typically based on manual or compiler-based transformations. This paper presents a pattern and library-based approach to optimizing collective communication in parallel high-performance applications, based on using non-blocking collective operations to enable overlapping of communication and computation. Common communication and computation patterns in iterative {SPMD} computations are used to motivate the transformations we present. Our approach provides the programmer with the capability to separately optimize communication and computation in an application, while automating the interaction between computation and communication to achieve maximum overlap. Performance results results with two model applications show more than 90\% decrease in communication overhead, resulting in 16\% and 21\% overall performance improvements.},
	eventtitle = {{SPAA}08: 20th {ACM} Symposium on Parallelism in Algorithms and Architectures},
	pages = {113--115},
	booktitle = {Proceedings of the twentieth annual symposium on Parallelism in algorithms and architectures},
	publisher = {{ACM}},
	author = {Hoefler, Torsten and Gottschling, Peter and Lumsdaine, Andrew},
	urldate = {2023-05-01},
	date = {2008-06-14},
	langid = {english},
	file = {Hoefler et al. - 2008 - Leveraging non-blocking collective communication i.pdf:/Users/npvietkhoa/Zotero/storage/GKNSJS6A/Hoefler et al. - 2008 - Leveraging non-blocking collective communication i.pdf:application/pdf},
}

@article{becker_patterns_nodate,
	title = {Patterns for Overlapping Communication and Computation},
	abstract = {Parallel applications commonly face the problem of sitting idle while waiting for remote data to become available. Even for problems where plenty of parallelism is available and good load balance is achievable, performance may be disappointing if local work cannot be overlapped with communication. We describe three patterns for achieving the overlap of communication with computation: overdecomposition, non-blocking communication, and speculation.},
	author = {Becker, Aaron and Venkataraman, Ramprasad and Kale, Laxmikant V},
	langid = {english},
	file = {Becker et al. - Patterns for Overlapping Communication and Computa.pdf:/Users/npvietkhoa/Zotero/storage/FRZ4UIL7/Becker et al. - Patterns for Overlapping Communication and Computa.pdf:application/pdf},
}

@online{noauthor_elsevier_nodate,
	title = {Elsevier Enhanced Reader},
	url = {https://reader.elsevier.com/reader/sd/pii/S0167739X09000697?token=FE682DE93832CEE4FCDD247E29C40673B55A3C3762E2A4F30B25B321B1EC9CA408CA613689CB08C7B57872FAF9CCC243&originRegion=eu-west-1&originCreation=20230501235557},
	urldate = {2023-05-01},
	langid = {english},
	doi = {10.1016/j.future.2009.05.017},
	file = {Full Text PDF:/Users/npvietkhoa/Zotero/storage/5CTGU7VT/Elsevier Enhanced Reader.pdf:application/pdf},
}

@article{feautrier_efficient_1992,
	title = {Some efficient solutions to the affine scheduling problem. I. One-dimensional time},
	volume = {21},
	issn = {1573-7640},
	url = {https://doi.org/10.1007/BF01407835},
	doi = {10.1007/BF01407835},
	abstract = {Programs and systems of recurrence equations may be represented as sets of actions which are to be executed subject to precedence constraints. In may cases, actions may be labelled by integral vectors in some iterations domains, and precedence constraints may be described by affine relations. A schedule for such a program is a function which assigns an execution data to each action. Knowledge of such a schedule allows one to estimate the intrinsic degree of parallelism of the program and to compile a parallel version for multiprocessor architectures or systolic arrays. This paper deals with the problem of finding closed form schedules as affine or piecewise affine functions of the iteration vector. An algorithm is presented which reduces the scheduling problem to a parametric linear program of small size, which can be readily solved by an efficient algorithm.},
	pages = {313--347},
	number = {5},
	journaltitle = {International Journal of Parallel Programming},
	shortjournal = {Int J Parallel Prog},
	author = {Feautrier, Paul},
	urldate = {2023-05-03},
	date = {1992-10-01},
	langid = {english},
	keywords = {Automatic parallelization, automatic systolic array design, scheduling},
	file = {Full Text PDF:/Users/npvietkhoa/Zotero/storage/V8NLUMB5/Feautrier - 1992 - Some efficient solutions to the affine scheduling .pdf:application/pdf},
}

@article{gabriel_towards_2010,
	title = {Towards performance portability through runtime adaptation for high-performance computing applications},
	volume = {22},
	issn = {1532-0634},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpe.1586},
	doi = {10.1002/cpe.1586},
	abstract = {The Abstract Data and Communication Library ({ADCL}) is an adaptive communication library optimizing application level group communication operations at runtime. The library provides for a given communication pattern a large number of implementations and incorporates a runtime selection logic in order to choose the implementation leading to the highest performance. In this paper, we demonstrate how an application utilizing {ADCL} is deployed on a wide range of {HPC} architectures, including an {IBM} Blue Gene/L, an {NEC} {SX}-8, an {IBM} Power {PC} cluster using an {IBM} Federation Switch, an {AMD} Opteron cluster utilizing a 4xInfiniBand and a Gigabit Ethernet network, and an Intel {EM}64T cluster using a hierarchical Gigabit Ethernet network with reduced uplink bandwidth. We demonstrate, how different implementations for the three-dimensional neighborhood communication lead to the minimal execution time of the application on different architectures. {ADCL} gives the user the advantage of having to maintain only a single version of the source code and still have the ability to achieve close to optimal performance for the application on all architectures. Copyright © 2010 John Wiley \& Sons, Ltd.},
	pages = {2230--2246},
	number = {16},
	journaltitle = {Concurrency and Computation: Practice and Experience},
	author = {Gabriel, Edgar and Feki, Saber and Benkert, Katharina and Resch, Michael M.},
	urldate = {2023-05-03},
	date = {2010},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/cpe.1586},
	keywords = {collective communication, runtime adaption},
	file = {Full Text PDF:/Users/npvietkhoa/Zotero/storage/4M4KCJYT/Gabriel et al. - 2010 - Towards performance portability through runtime ad.pdf:application/pdf;Snapshot:/Users/npvietkhoa/Zotero/storage/Z56H6E4F/cpe.html:text/html},
}

@inproceedings{ahmed_petal_2016,
	location = {Cham},
	title = {Petal Tool for Analyzing and Transforming Legacy {MPI} Applications},
	isbn = {978-3-319-29778-1},
	doi = {10.1007/978-3-319-29778-1_10},
	series = {Lecture Notes in Computer Science},
	abstract = {Legacy {MPI} applications are an important and economically valuable category of parallel software that rely on the {MPI}-1, {MPI}-2 (and, more recently, {MPI}-3) standards to achieve performance and portability. Many of these applications have been developed or ported to {MPI} over the past two decades, with the implicit (dual) goal of achieving acceptably high performance and scalability, and a high level of portability between diverse parallel architectures. However they were often created implicitly using {MPI} in ways that exploited how a particular underlying {MPI} behaved at the time (such as those with polling progress and poor implementation of some operations). Thus, they did not necessarily take advantage of the full potential for describing latent concurrency or for loosening the coupling of the application thread from the message scheduling and transfer.},
	pages = {156--170},
	booktitle = {Languages and Compilers for Parallel Computing},
	publisher = {Springer International Publishing},
	author = {Ahmed, Hadia and Skjellum, Anthony and Pirkelbauer, Peter},
	editor = {Shen, Xipeng and Mueller, Frank and Tuck, James},
	date = {2016},
	langid = {english},
	keywords = {Abstract Syntax Tree, Message Passing Interface, Alias Analysis, Message Buffer, Message Passing Interface Process},
	file = {Full Text PDF:/Users/npvietkhoa/Zotero/storage/YS5T9ZT4/Ahmed et al. - 2016 - Petal Tool for Analyzing and Transforming Legacy M.pdf:application/pdf},
}

@book{cardellini_overlapping_2016,
	title = {Overlapping Communication with Computation in {MPI} Applications},
	abstract = {In High Performance Computing ({HPC}), minimizing communication overhead is one of the most important goals in order to get high performance. This is more than ever important on exascale platforms, where there will be a much higher degree of parallelism compared to petascale platforms, resulting in increased communication overhead with considerable impact on application execution time and energy expenses. A good strategy for containing this overhead is to hide communication costs by overlapping them with computation. Despite the increasing interest in achieving computation/communication overlapping, details about the reasons that prevent it from succeeding are not easy to find, leading to confusion and poor application optimization. The Message Passing Interface ({MPI}) library, a de-facto standard in the {HPC} world, has always provided non-blocking communication routines able, in theory, to achieve communica-tion/computation overlapping. Unfortunately, several factors related with the {MPI} independent progress and offload capability of the underlying network, make this overlap hard do achieve. With the introduction of one-sided communication routines, providing high quality {MPI} implementations , able to progress communication independently, is becoming as important as providing low latency and high bandwidth communication. In this paper, we gather the most significant contributions about computation/communi-cation overlapping and provide technical explanation of how such overlap can be achieved on modern supercomputers.},
	author = {Cardellini, Valeria and Fanfarillo, Alessandro and Filippone, Salvatore},
	date = {2016-02-16},
	file = {Full Text PDF:/Users/npvietkhoa/Zotero/storage/8KLV4KZE/Cardellini et al. - 2016 - Overlapping Communication with Computation in MPI .pdf:application/pdf},
}

@inproceedings{weihang_jiang_high_2004,
	location = {Chicago, {IL}, {USA}},
	title = {High performance {MPI}-2 one-sided communication over {InfiniBand}},
	isbn = {978-0-7803-8430-9},
	url = {https://ieeexplore.ieee.org/document/1336648/},
	doi = {10.1109/CCGrid.2004.1336648},
	abstract = {Many existing {MPI}-2 one-sided communication implementations are built on top of {MPI} send/receive operations. Although this approach can achieve good portability, it suffers from high communication overhead and dependency on remote process for communication progress. To address these problems, we propose a high performance {MPI}-2 onesided communication design over the {InﬁniBand} Architecture. In our design, {MPI}-2 one-sided communication operations such as {MPI} Put, {MPI} Get and {MPI} Accumulate are directly mapped to {InﬁniBand} Remote Direct Memory Access ({RDMA}) operations.},
	eventtitle = {2004 {IEEE} International Symposium on Cluster Computing and the Grid},
	pages = {531--538},
	booktitle = {{IEEE} International Symposium on Cluster Computing and the Grid, 2004. {CCGrid} 2004.},
	publisher = {{IEEE}},
	author = {{Weihang Jiang} and {Jiuxing Liu} and {Hyun-Wook Jin} and Panda, D.K. and Gropp, W. and Thakur, R.},
	urldate = {2023-05-03},
	date = {2004},
	langid = {english},
	file = {Weihang Jiang et al. - 2004 - High performance MPI-2 one-sided communication ove.pdf:/Users/npvietkhoa/Zotero/storage/B8LJDXRJ/Weihang Jiang et al. - 2004 - High performance MPI-2 one-sided communication ove.pdf:application/pdf},
}

@article{prema_study_2019,
	title = {A study on popular auto-parallelization frameworks},
	volume = {31},
	issn = {1532-0634},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpe.5168},
	doi = {10.1002/cpe.5168},
	abstract = {We study five popular auto-parallelization frameworks (Cetus, Par4all, Rose, {ICC}, and Pluto) and compare them qualitatively as well as quantitatively. All the frameworks primarily deal with loop parallelization but differ in the techniques used to identify parallelization opportunities. Due to this variance, various aspects, such as certain loop transformations, are supported only in a few frameworks. The frameworks exhibit varying abilities in handling loop-carried dependence and, therefore, achieve different amounts of speedup on widely used {PolyBench} and {NAS} parallel benchmarks. In particular, Intel C Compiler ({ICC}) fares as an overall good parallelizer. Our study also highlights the need for more sophisticated analyses, user-driven parallelization, and meta-auto-parallelizer that provides combined benefits of various frameworks.},
	pages = {e5168},
	number = {17},
	journaltitle = {Concurrency and Computation: Practice and Experience},
	author = {Prema, S. and Nasre, Rupesh and Jehadeesan, R. and Panigrahi, B.k.},
	urldate = {2023-05-03},
	date = {2019},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/cpe.5168},
	keywords = {loop transformations, loop-carried dependence, privatization, vectorization},
	file = {Full Text PDF:/Users/npvietkhoa/Zotero/storage/IV69SC9L/Prema et al. - 2019 - A study on popular auto-parallelization frameworks.pdf:application/pdf;Snapshot:/Users/npvietkhoa/Zotero/storage/9VDTPZGW/cpe.html:text/html},
}

@inproceedings{hoefler_case_2007,
	location = {Berlin, Heidelberg},
	title = {A Case for Standard Non-blocking Collective Operations},
	isbn = {978-3-540-75416-9},
	doi = {10.1007/978-3-540-75416-9_22},
	series = {Lecture Notes in Computer Science},
	abstract = {In this paper we make the case for adding standard non-blocking collective operations to the {MPI} standard. The non-blocking point-to-point and blocking collective operations currently defined by {MPI} provide important performance and abstraction benefits. To allow these benefits to be simultaneously realized, we present an application programming interface for non-blocking collective operations in {MPI}. Microbenchmark and application-based performance results demonstrate that non-blocking collective operations offer not only improved convenience, but improved performance as well, when compared to manual use of threads with blocking collectives.},
	pages = {125--134},
	booktitle = {Recent Advances in Parallel Virtual Machine and Message Passing Interface},
	publisher = {Springer},
	author = {Hoefler, Torsten and Kambadur, Prabhanjan and Graham, Richard L. and Shipman, Galen and Lumsdaine, Andrew},
	editor = {Cappello, Franck and Herault, Thomas and Dongarra, Jack},
	date = {2007},
	langid = {english},
	keywords = {Collective Operation, Message Passing Interface, Collective Communication, Communication Overhead, Parallel Virtual Machine},
	file = {Full Text PDF:/Users/npvietkhoa/Zotero/storage/542IZGXJ/Hoefler et al. - 2007 - A Case for Standard Non-blocking Collective Operat.pdf:application/pdf},
}

@article{bondhugula_practical_2008,
	title = {A practical automatic polyhedral parallelizer and locality optimizer},
	volume = {43},
	issn = {0362-1340, 1558-1160},
	url = {https://dl.acm.org/doi/10.1145/1379022.1375595},
	doi = {10.1145/1379022.1375595},
	abstract = {We present the design and implementation of an automatic polyhedral source-to-source transformation framework that can optimize regular programs (sequences of possibly imperfectly nested loops) for parallelism and locality simultaneously. Through this work, we show the practicality of analytical model-driven automatic transformation in the polyhedral model -- far beyond what is possible by current production compilers. Unlike previous works, our approach is an end-to-end fully automatic one driven by an integer linear optimization framework that takes an explicit view of finding good ways of tiling for parallelism and locality using affine transformations. The framework has been implemented into a tool to automatically generate {OpenMP} parallel code from C program sections. Experimental results from the tool show very high speedups for local and parallel execution on multi-cores over state-of-the-art compiler frameworks from the research community as well as the best native production compilers. The system also enables the easy use of powerful empirical/iterative optimization for general arbitrarily nested loop sequences.},
	pages = {101--113},
	number = {6},
	journaltitle = {{ACM} {SIGPLAN} Notices},
	shortjournal = {{SIGPLAN} Not.},
	author = {Bondhugula, Uday and Hartono, Albert and Ramanujam, J. and Sadayappan, P.},
	urldate = {2023-05-04},
	date = {2008-05-30},
	langid = {english},
	file = {Full Text PDF:/Users/npvietkhoa/Zotero/storage/RPN6SILY/Bondhugula et al. - 2008 - A practical automatic polyhedral parallelizer and .pdf:application/pdf},
}

@article{hoeer_optimizing_nodate,
	title = {Optimizing a Conjugate Gradient Solver with Non-Blocking Collective Operations},
	abstract = {This paper presents a case study that analyzes the suitability and usage of nonblocking collective operations in parallel applications. As with their point-to-point counterparts, non-blocking collective operations provide the ability to overlap communication with computation and to avoid unnecessary synchronization. These operations are provided for {MPI} programs with {LibNBC}, a portable low-overhead implementation of non-blocking collective operations built on {MPI}-1. The straightforward applicability of the {LibNBC} is demonstrated by incorporating non-blocking collective operations into a parallel conjugate gradient solver. Although only minor changes are required to use them, non-blocking collective operations allow most of the communication costs to be hidden and provide performance improvements of up to 34\%. We also show that, because of overlap, there is no signiﬁcant performance diﬀerence between Gigabit Ethernet and {InﬁniBandTM} for special cases of our calculation.},
	author = {Hoeﬂer, Torsten and Gottschling, Peter and Lumsdaine, Andrew and Rehm, Wolfgang},
	langid = {english},
	file = {Hoeﬂer et al. - Optimizing a Conjugate Gradient Solver with Non-Bl.pdf:/Users/npvietkhoa/Zotero/storage/DH6VF4NK/Hoeﬂer et al. - Optimizing a Conjugate Gradient Solver with Non-Bl.pdf:application/pdf},
}

@article{liu_computation-communication_nodate,
	title = {Computation-Communication Overlap on Network-of-Workstation Multiprocessors},
	abstract = {This paper describes and evaluates a compiler transformation that improves the performance of parallel programs on Network-of-Workstation ({NOW}) sharedmemory multiprocessors. The transformation overlaps the communication time resulting form non-local memory accesses with the computation time in parallel loops to effectively hide the latency of the remote accesses. The transformation peels from a parallel loop iterations that access remote data and re-schedules them after the execution of iterations that access only local data (localonly iterations). Asynchronous prefetching of remote data is used to overlap non-local access latency with the execution of local-only iterations. Experimental evaluation of the transformation on a {NOW} multiprocessor indicates that it is generally effective in improving parallel execution time (up to 1.9 times). The extent of the beneﬁt is determined by three factors: the size of localonly computations, the signiﬁcance of remote memory access latency, and the position of the iterations that access remote data in a parallel loop.},
	author = {Liu, Gary and Abdelrahman, Tarek S},
	langid = {english},
	file = {Liu and Abdelrahman - Computation-Communication Overlap on Network-of-Wo.pdf:/Users/npvietkhoa/Zotero/storage/HSHD6WH3/Liu and Abdelrahman - Computation-Communication Overlap on Network-of-Wo.pdf:application/pdf},
}

@article{brightwell_analyzing_2005,
	title = {Analyzing the Impact of Overlap, Offload, and Independent Progress for Message Passing Interface Applications.},
	volume = {19},
	abstract = {The overlap of computation and communication has long been considered to be a significant performance benefit for applications. Similarly, the ability of the Message Passing Interface ({MPI}) to make independent progress (that is, to make progress on outstanding communication operations while not in the {MPI} library) is also believed to yield performance benefits. Using an intelligent network interface to offload the work required to support overlap and independent progress is thought to be an ideal solu- tion, but the benefits of this approach have not been stud- ied in depth at the application level. This lack of analysis is complicated by the fact that most {MPI} implementations do not sufficiently support overlap or independent progress. Recent work has demonstrated a quantifiable advantage for an {MPI} implementation that uses offload to provide overlap and independent progress. The study is conducted on two different platforms with each having two {MPI} implementations (one with and one without inde- pendent progress). Thus, identical network hardware and virtually identical software stacks are used. Furthermore, one platform, {ASCI} Red, allows further separation of fea- tures such as overlap and offload. Thus, this paper extends previous work by further qualifying the source of the performance advantage: offload, overlap, or inde- pendent progress.},
	pages = {103--117},
	journaltitle = {{IJHPCA}},
	shortjournal = {{IJHPCA}},
	author = {Brightwell, Ron and Riesen, Rolf and Underwood, Keith},
	date = {2005-01-01},
	file = {Full Text PDF:/Users/npvietkhoa/Zotero/storage/MT972G9Z/Brightwell et al. - 2005 - Analyzing the Impact of Overlap, Offload, and Inde.pdf:application/pdf},
}

@book{hromkovic_communication_1997,
	title = {Communication Complexity and Parallel Computing},
	isbn = {978-3-540-57459-0},
	abstract = {The communication complexity of two-party protocols is an only 15 years old complexity measure, but it is already considered to be one of the fundamen tal complexity measures of recent complexity theory. Similarly to Kolmogorov complexity in the theory of sequential computations, communication complex ity is used as a method for the study of the complexity of concrete computing problems in parallel information processing. Especially, it is applied to prove lower bounds that say what computer resources (time, hardware, memory size) are necessary to compute the given task. Besides the estimation of the compu tational difficulty of computing problems the proved lower bounds are useful for proving the optimality of algorithms that are already designed. In some cases the knowledge about the communication complexity of a given problem may be even helpful in searching for efficient algorithms to this problem. The study of communication complexity becomes a well-defined indepen dent area of complexity theory. In addition to a strong relation to several funda mental complexity measures (and so to several fundamental problems of com plexity theory) communication complexity has contributed to the study and to the understanding of the nature of determinism, nondeterminism, and random ness in algorithmics. There already exists a non-trivial mathematical machinery to handle the communication complexity of concrete computing problems, which gives a hope that the approach based on communication complexity will be in strumental in the study of several central open problems of recent complexity theory.},
	pagetotal = {360},
	publisher = {Springer Science \& Business Media},
	author = {Hromkovič, Juraj},
	date = {1997-02-26},
	langid = {english},
	keywords = {Computers / Computer Science, Computers / Data Transmission Systems / Electronic Data Interchange, Computers / Information Technology, Computers / Networking / General, Computers / Networking / Hardware, Computers / Software Development \& Engineering / Systems Analysis \& Design, Mathematics / Discrete Mathematics, Mathematics / Logic, Technology \& Engineering / Data Transmission Systems / General, Technology \& Engineering / Telecommunications},
}

@article{gross_communication_1994,
	title = {Communication styles for parallel systems},
	volume = {27},
	issn = {0018-9162},
	url = {https://www.computer.org/csdl/magazine/co/1994/12/rz034/13rRUxBJhAD},
	doi = {10.1109/2.335726},
	abstract = {Distributed-memory parallel systems rely on explicit message exchange for communication, but the communication operations they support can differ in many aspects. One key difference is the way messages are generated or consumed. With systolic communication, a message is transmitted as it is generated. For example, the result computed by the multiplier is sent directly to the communication subsystem for transmission to another node. With memory communication, the complete message is generated and stored in memory, and then transmitted to its destination. Since sender and receiver nodes are individually controlled, they can use different communication styles. One example of memory communication is message passing: both the sender and receiver buffer the message in memory. These two communication styles place different demands on processor design. This article illustrates each style's effect on processor resources for some key application kernels. We are targeting the {iWarp} system because it supports both communication styles. Two parallel-program generators, one for each communication style, automatically map the sample programs.},
	pages = {34--44},
	number = {12},
	journaltitle = {Computer},
	author = {Gross, Thomas and Hinrichs, Susan and O'Hallaron, David R. and Stricker, Thomas and Hasegawa, Atsushi},
	urldate = {2023-05-19},
	date = {1994-12-01},
	note = {Publisher: {IEEE} Computer Society},
}

@inproceedings{davis_efficient_1997,
	location = {Berlin, Heidelberg},
	title = {Efficient communication mechanisms for cluster based parallel computing},
	isbn = {978-3-540-68085-7},
	doi = {10.1007/3-540-62573-9_1},
	series = {Lecture Notes in Computer Science},
	abstract = {The key to crafting an effective scalable parallel computing system lies in minimizing the delays imposed by the system. Of particular importance are communications delays, since parallel algorithms must communicate frequently. The communication delay is a system-imposed latency. The existence of relatively inexpensive high performance workstations and emerging high performance interconnect options provide compelling economic motivation to investigate {NOW}/{COW} (network/cluster of workstation) architectures. However, these commercial components have been designed for generality. Cluster nodes are connected by longer physical wire paths than found in special-purpose supercomputer systems. Both effects tend to impose intractable latencies on communication. Even larger system-imposed delays result from the overhead of sending and receiving messages. This overhead can come in several forms, including {CPU} occupancy by protocol and device code as well as interference with {CPU} access to various levels of the memory hierarchy. Access contention becomes even more onerous when the nodes in the system are themselves symmetric multiprocessors. Additional delays are incurred if the communication mechanism requires processes to run concurrently in order to communicate with acceptable efficiency. This paper presents the approach taken by the Utah Avalanche project which spans user level code, operating system support, and network interface hardware. The result minimizes the constraining effects of latency, overhead, and loosely coupled scheduling that are common characteristics in {NOW}-based architectures.},
	pages = {1--15},
	booktitle = {Communication and Architectural Support for Network-Based Parallel Computing},
	publisher = {Springer},
	author = {Davis, Al and Swanson, Mark and Parker, Mike},
	editor = {Panda, Dhabaleswar K. and Stunkel, Craig B.},
	date = {1997},
	langid = {english},
	keywords = {Cache Line, Memory System, Message Passing, Network Interface, System Call},
	file = {Full Text PDF:/Users/npvietkhoa/Zotero/storage/Y42T28TN/Davis et al. - 1997 - Efficient communication mechanisms for cluster bas.pdf:application/pdf},
}

@article{wan_efficient_2023,
	title = {An efficient communication strategy for massively parallel computation in {CFD}},
	volume = {79},
	issn = {1573-0484},
	url = {https://doi.org/10.1007/s11227-022-04940-3},
	doi = {10.1007/s11227-022-04940-3},
	abstract = {With the development of high-performance computers, it is necessary to develop efficient parallel algorithms in the field of computational fluid dynamics ({CFD}). In this study, a novel parallel communication strategy based on asynchronous and packaged communication is proposed. The strategy implements an aggregated communication process, which requires only one communication in each iteration step, significantly reducing the number of communications. The correctness and convergence of the novel strategy are demonstrated from both theoretical and experimental perspectives. And based on the real vehicle {CHN}-T model with 140 million meshes, a detailed performance comparison and analysis is performed for the novel strategy and the traditional strategy, showing that the novel strategy has significant advantages in terms of scalability. Finally, the strong scalability and weak scalability tests are carried out separately for the {CHN}-T model. The strong scaling efficiency can reach 74\% with 10.5 billion meshes and 256,000 cores. The weak scaling parallel efficiency can reach 90\% with 10 billion meshes and 179,000 cores. This research work has laid an important foundation for the development of the fast design of aircraft and cutting-edge numerical methods.},
	pages = {7560--7583},
	number = {7},
	journaltitle = {The Journal of Supercomputing},
	shortjournal = {J Supercomput},
	author = {Wan, {YunBo} and He, Lei and Zhang, Yong and Zhao, Zhong and Liu, Jie and Zhang, {HaoYuan}},
	urldate = {2023-05-21},
	date = {2023-05-01},
	langid = {english},
	keywords = {Asynchronous communication, {CFD}, Massively parallel computing, Scalability, Tens of billion grid},
	file = {Full Text PDF:/Users/npvietkhoa/Zotero/storage/J5GHPIDL/Wan et al. - 2023 - An efficient communication strategy for massively .pdf:application/pdf},
}

@online{noauthor_doi101016s0743-73150300008-x_nodate,
	title = {doi:10.1016/S0743-7315(03)00008-X {\textbar} Elsevier Enhanced Reader},
	url = {https://reader.elsevier.com/reader/sd/pii/S074373150300008X?token=95647AA8C0A087818F9C66D147657B2FEC5F025F88151008F28A7545789D3617922E7921269CAEE8350481265C1B4BCF&originRegion=eu-west-1&originCreation=20230521222820},
	shorttitle = {doi},
	urldate = {2023-05-21},
	langid = {english},
	doi = {10.1016/S0743-7315(03)00008-X},
	file = {Full Text PDF:/Users/npvietkhoa/Zotero/storage/X2ZFBAJS/doi10.1016S0743-7315(03)00008-X  Elsevier Enhan.pdf:application/pdf},
}

@inproceedings{sancho_mpi_2006,
	location = {Tampa, Florida},
	title = {{MPI} tools and performance studies---Quantifying the potential benefit of overlapping communication and computation in large-scale scientific applications},
	isbn = {978-0-7695-2700-0},
	url = {http://portal.acm.org/citation.cfm?doid=1188455.1188585},
	doi = {10.1145/1188455.1188585},
	eventtitle = {the 2006 {ACM}/{IEEE} conference},
	pages = {125},
	booktitle = {Proceedings of the 2006 {ACM}/{IEEE} conference on Supercomputing  - {SC} '06},
	publisher = {{ACM} Press},
	author = {Sancho, José Carlos and Barker, Kevin J. and Kerbyson, Darren J. and Davis, Kei},
	urldate = {2023-05-23},
	date = {2006},
	langid = {english},
	file = {Full Text PDF:/Users/npvietkhoa/Zotero/storage/KSEFG497/Sancho et al. - 2006 - MPI tools and performance studies---Quantifying th.pdf:application/pdf},
}

@collection{shen_languages_2016,
	location = {Cham},
	title = {Languages and Compilers for Parallel Computing: 28th International Workshop, {LCPC} 2015, Raleigh, {NC}, {USA}, September 9-11, 2015, Revised Selected Papers},
	volume = {9519},
	isbn = {978-3-319-29777-4 978-3-319-29778-1},
	url = {http://link.springer.com/10.1007/978-3-319-29778-1},
	series = {Lecture Notes in Computer Science},
	shorttitle = {Languages and Compilers for Parallel Computing},
	publisher = {Springer International Publishing},
	editor = {Shen, Xipeng and Mueller, Frank and Tuck, James},
	urldate = {2023-05-27},
	date = {2016},
	langid = {english},
	doi = {10.1007/978-3-319-29778-1},
	keywords = {Automatic parallelization, Communication, Concurrency, Correctness and reliability, Data centers, {GPU}, Graph algorithms, High performance computing, Latency, Mobile computing, Optimization, Optimizing compilers, Parallel algorithms, Parallel applications, Parallel computing, Parallel programming languages, Parallelizing compiler, Programming models, Scientific computing, Vectorization},
	file = {Full Text PDF:/Users/npvietkhoa/Zotero/storage/A6YM6M4Y/Shen et al. - 2016 - Languages and Compilers for Parallel Computing 28.pdf:application/pdf},
}

@collection{kranzlmuller_recent_2004,
	location = {Berlin, Heidelberg},
	title = {Recent Advances in Parallel Virtual Machine and Message Passing Interface: 11th European {PVM}/{MPI} Users’ Group Meeting Budapest, Hungary, September 19 - 22, 2004. Proceedings},
	volume = {3241},
	isbn = {978-3-540-23163-9 978-3-540-30218-6},
	url = {http://link.springer.com/10.1007/b100820},
	series = {Lecture Notes in Computer Science},
	shorttitle = {Recent Advances in Parallel Virtual Machine and Message Passing Interface},
	publisher = {Springer},
	editor = {Kranzlmüller, Dieter and Kacsuk, Péter and Dongarra, Jack},
	editorb = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard},
	editorbtype = {redactor},
	urldate = {2023-05-28},
	date = {2004},
	langid = {english},
	doi = {10.1007/b100820},
	keywords = {{OpenMP}, grid computing, algorithms, cluster computing, data structure, data structures, distributed computing, grid middleware, Hardware, message passing interface, mpi, parallel algorithms, programming, pvm, Scheduling},
	file = {Full Text PDF:/Users/npvietkhoa/Zotero/storage/C78BBFNR/Kranzlmüller et al. - 2004 - Recent Advances in Parallel Virtual Machine and Me.pdf:application/pdf},
}

@inproceedings{jiang_efficient_2004,
	location = {Berlin, Heidelberg},
	title = {Efficient Implementation of {MPI}-2 Passive One-Sided Communication on {InfiniBand} Clusters},
	isbn = {978-3-540-30218-6},
	doi = {10.1007/978-3-540-30218-6_16},
	series = {Lecture Notes in Computer Science},
	abstract = {In this paper we compare various design alternatives for synchronization in {MPI}-2 passive one-sided communication on {InfiniBand} clusters. We discuss several requirements for synchronization in passive one-sided communication. Based on these requirements, we present four design alternatives, which can be classified into two categories: thread-based and atomic operation-based. In thread-based designs, synchronization is achieved with the help of extra threads. In atomic operation-based designs, we exploit {InfiniBand} atomic operations such as Compare-and-Swap and Fetch-and-Add. Our performance evaluation results show that the atomic operation-based design can require less synchronization overhead, achieve better concurrency, and consume fewer computing resources compared with the thread based design.},
	pages = {68--76},
	booktitle = {Recent Advances in Parallel Virtual Machine and Message Passing Interface},
	publisher = {Springer},
	author = {Jiang, Weihang and Liu, Jiuxing and Jin, Hyun-Wook and Panda, Dhabaleswar K. and Buntinas, Darius and Thakur, Rajeev and Gropp, William D.},
	editor = {Kranzlmüller, Dieter and Kacsuk, Péter and Dongarra, Jack},
	date = {2004},
	langid = {english},
	keywords = {Message Passing Interface, Atomic Operation, Origin Process, Remote Memory, Target Process},
	file = {Full Text PDF:/Users/npvietkhoa/Zotero/storage/2UKWCRLP/Jiang et al. - 2004 - Efficient Implementation of MPI-2 Passive One-Side.pdf:application/pdf},
}

@inproceedings{rabenseifner_more_2004,
	location = {Berlin, Heidelberg},
	title = {More Efficient Reduction Algorithms for Non-Power-of-Two Number of Processors in Message-Passing Parallel Systems},
	isbn = {978-3-540-30218-6},
	doi = {10.1007/978-3-540-30218-6_13},
	series = {Lecture Notes in Computer Science},
	abstract = {We present improved algorithms for global reduction operations for message-passing systems. Each of p processors has a vector of m data items, and we want to compute the element-wise “sum” under a given, associative function of the p vectors. The result, which is also a vector of m items, is to be stored at either a given root processor ({MPI}\_Reduce), or all p processors ({MPI}\_Allreduce). A further constraint is that for each data item and each processor the result must be computed in the same order, and with the same bracketing. Both problems can be solved in O(m+log2p) communication and computation time. Such reduction operations are part of {MPI} (the Message Passing Interface), and the algorithms presented here achieve significant improvements over currently implemented algorithms for the important case where p is not a power of 2. Our algorithm requires ⌈log2p⌉ + 1 rounds – one round off from optimal – for small vectors. For large vectors twice the number of rounds is needed, but the communication and computation time is less than 3mβ and 3/2mγ, respectively, an improvement from 4mβ and 2mγ achieved by previous algorithms (with the message transfer time modeled as α + mβ, and reduction-operation execution time as mγ). For p=3× 2nand p=9× 2nand small m ≤ b for some threshold b, and p=q 2nwith small q, our algorithm achieves the optimal ⌈log2p⌉ number of rounds.},
	pages = {36--46},
	booktitle = {Recent Advances in Parallel Virtual Machine and Message Passing Interface},
	publisher = {Springer},
	author = {Rabenseifner, Rolf and Träff, Jesper Larsson},
	editor = {Kranzlmüller, Dieter and Kacsuk, Péter and Dongarra, Jack},
	date = {2004},
	langid = {english},
	keywords = {Message Passing Interface, Exchange Step, Reduction Algorithm, Reduction Phase, Result Vector},
	file = {Full Text PDF:/Users/npvietkhoa/Zotero/storage/CRZXQP9T/Rabenseifner and Träff - 2004 - More Efficient Reduction Algorithms for Non-Power-.pdf:application/pdf},
}

@inproceedings{traff_verifying_2004,
	location = {Berlin, Heidelberg},
	title = {Verifying Collective {MPI} Calls},
	isbn = {978-3-540-30218-6},
	doi = {10.1007/978-3-540-30218-6_11},
	series = {Lecture Notes in Computer Science},
	abstract = {The collective communication operations of {MPI}, and in general {MPI} operations with non-local semantics, require the processes participating in the calls to provide consistent parameters, eg. a unique root process, matching type signatures and amounts for data to be exchanged, or same operator. Under normal use of {MPI} such exhaustive consistency checks are typically too expensive to perform and would compromise optimizations for high performance in the collective routines. However, confusing and hard-to-find errors (deadlocks, wrong results, or program crash) can happen by inconsistent calls to collective operations.},
	pages = {18--27},
	booktitle = {Recent Advances in Parallel Virtual Machine and Message Passing Interface},
	publisher = {Springer},
	author = {Träff, Jesper Larsson and Worringen, Joachim},
	editor = {Kranzlmüller, Dieter and Kacsuk, Péter and Dongarra, Jack},
	date = {2004},
	langid = {english},
	keywords = {Collective Operation, Communication Operation, Consistent Parameter, Error Handler, Virtual Topology},
	file = {Full Text PDF:/Users/npvietkhoa/Zotero/storage/9ZWLXLGH/Träff and Worringen - 2004 - Verifying Collective MPI Calls.pdf:application/pdf},
}

@inproceedings{thakur_minimizing_2004,
	location = {Berlin, Heidelberg},
	title = {Minimizing Synchronization Overhead in the Implementation of {MPI} One-Sided Communication},
	isbn = {978-3-540-30218-6},
	doi = {10.1007/978-3-540-30218-6_15},
	series = {Lecture Notes in Computer Science},
	abstract = {The one-sided communication operations in {MPI} areintended to provide the convenience of directly accessing remote memory and the potential for higher performance than regular point-to-point communication. Our performance measurements with three {MPI} implementations ({IBM} {MPI}, Sun {MPI}, and {LAM}) indicate, however, that one-sided communication can perform much worse than point-to-point communication if the associated synchronization calls are not implemented efficiently. In this paper, we describe our efforts to minimize the overhead of synchronization in our implementation of one-sided communication in {MPICH}-2. We describe our optimizations for all three synchronization mechanisms defined in {MPI}: fence, post-start-complete-wait, and lock-unlock. Our performance results demonstrate that, for short messages, {MPICH}-2 performs six times faster than {LAM} for fence synchronization and 50\% faster for post-start-complete-wait synchronization, and it performs more than twice as fast as Sun {MPI} for all three synchronization methods.},
	pages = {57--67},
	booktitle = {Recent Advances in Parallel Virtual Machine and Message Passing Interface},
	publisher = {Springer},
	author = {Thakur, Rajeev and Gropp, William D. and Toonen, Brian},
	editor = {Kranzlmüller, Dieter and Kacsuk, Péter and Dongarra, Jack},
	date = {2004},
	langid = {english},
	keywords = {Message Passing Interface, Origin Process, Message Size, Short Message, Synchronization Overhead},
	file = {Full Text PDF:/Users/npvietkhoa/Zotero/storage/2G828KTT/Thakur et al. - 2004 - Minimizing Synchronization Overhead in the Impleme.pdf:application/pdf},
}

@inproceedings{barchet-estefanel_fast_2004,
	location = {Berlin, Heidelberg},
	title = {Fast Tuning of Intra-cluster Collective Communications},
	isbn = {978-3-540-30218-6},
	doi = {10.1007/978-3-540-30218-6_12},
	series = {Lecture Notes in Computer Science},
	abstract = {Recent works try to optimise collective communication in grid systems focusing mostly on the optimisation of communications among different clusters. We believe that intra-cluster collective communications should also be optimised, as a way to improve the overall efficiency and to allow the construction of multi-level collective operations. Indeed, inside homogeneous clusters, a simple optimisation approach rely on the comparison from different implementation strategies, through their communication models. In this paper we evaluate this approach, comparing different implementation strategies with their predicted performances. As a result, we are able to choose the communication strategy that better adapts to each network environment.},
	pages = {28--35},
	booktitle = {Recent Advances in Parallel Virtual Machine and Message Passing Interface},
	publisher = {Springer},
	author = {Barchet-Estefanel, Luiz Angelo and Mounié, Grégory},
	editor = {Kranzlmüller, Dieter and Kacsuk, Péter and Dongarra, Jack},
	date = {2004},
	langid = {english},
	keywords = {Collective Communication, Message Size, Communication Model, Implementation Strategy, Transmission Delay},
	file = {Full Text PDF:/Users/npvietkhoa/Zotero/storage/LKJJZZJF/Barchet-Estefanel and Mounié - 2004 - Fast Tuning of Intra-cluster Collective Communicat.pdf:application/pdf},
}

@inproceedings{szepieniec_tools_2004,
	location = {Berlin, Heidelberg},
	title = {Tools and Services for Interactive Applications on the Grid – The {CrossGrid} Tutorial},
	isbn = {978-3-540-30218-6},
	doi = {10.1007/978-3-540-30218-6_10},
	series = {Lecture Notes in Computer Science},
	abstract = {The {CrossGrid} project aims to develop new Grid services and tools for interactive compute- and data-intensive applications. This Tutorial comprises presentations and training exercises prepared to familiarize the user with the area of Grid computing being researched by the {CrossGrid}. We present tools aimed at both users and Grid application developers. The exercises cover many subjects, from user-friendly utilities for handling Grid jobs, through interactive monitoring of applications and infrastructure, to data access optimization mechanisms.},
	pages = {14--17},
	booktitle = {Recent Advances in Parallel Virtual Machine and Message Passing Interface},
	publisher = {Springer},
	author = {Szepieniec, Tomasz and Radecki, Marcin and Rycerz, Katarzyna and Bubak, Marian and Malawski, Maciej},
	editor = {Kranzlmüller, Dieter and Kacsuk, Péter and Dongarra, Jack},
	date = {2004},
	langid = {english},
	keywords = {Message Passing Interface, Grid Application, Grid Environment, Grid Service, Interactive Application},
	file = {Full Text PDF:/Users/npvietkhoa/Zotero/storage/DXFIA29D/Szepieniec et al. - 2004 - Tools and Services for Interactive Applications on.pdf:application/pdf},
}

@inproceedings{lusk_open_2004,
	location = {Berlin, Heidelberg},
	title = {An Open Cluster System Software Stack},
	isbn = {978-3-540-30218-6},
	doi = {10.1007/978-3-540-30218-6_6},
	series = {Lecture Notes in Computer Science},
	abstract = {By “cluster system software,” we mean the software that turns a collection of individual machines into a powerful resource for a wide variety of applications. In this talk we will examine one loosely integrated collection of open-source cluster system software that includes an infrastructure for building component-based systems management tools, a collection of components based on this infrastructure that has been used for the last year to manage a medium-sized cluster, a scalable process-management component in this collection that provides for both batch and interactive use, and an {MPI}-2 implementation together with debugging and performance analysis tools that help in developing advanced applications.},
	pages = {9--9},
	booktitle = {Recent Advances in Parallel Virtual Machine and Message Passing Interface},
	publisher = {Springer},
	author = {Lusk, Ewing},
	editor = {Kranzlmüller, Dieter and Kacsuk, Péter and Dongarra, Jack},
	date = {2004},
	langid = {english},
	file = {Full Text PDF:/Users/npvietkhoa/Zotero/storage/4GUV8YQW/Lusk - 2004 - An Open Cluster System Software Stack.pdf:application/pdf},
}

@inproceedings{volkert_austrian_2004,
	location = {Berlin, Heidelberg},
	title = {The Austrian Grid Initiative – High Level Extensions to Grid Middleware},
	isbn = {978-3-540-30218-6},
	doi = {10.1007/978-3-540-30218-6_2},
	series = {Lecture Notes in Computer Science},
	abstract = {The Austrian Grid initiative is the national effort of Austria to establish a nation-wide grid environment for computational science and research. The goal of the Austrian Grid is to pursue a variety of scientific users in utilizing the Grid for their applications, e.g. medical sciencs, high-energy physics, applied numerical simulations, astrophyscial simulations and solar observations, as well as meteorology and geophysics. All these applications rely on a wide range of diverse computer science technologies, composed from standard grid middleware and sophisticated high-level extensions, which enable the implementation and operation of the Austrian Grid testbed and its applications.},
	pages = {5--5},
	booktitle = {Recent Advances in Parallel Virtual Machine and Message Passing Interface},
	publisher = {Springer},
	author = {Volkert, Jens},
	editor = {Kranzlmüller, Dieter and Kacsuk, Péter and Dongarra, Jack},
	date = {2004},
	langid = {english},
	keywords = {Grid Application, Grid Service, Computational Science, Merical Simulation, National Effort},
	file = {Full Text PDF:/Users/npvietkhoa/Zotero/storage/GVTJYQ7G/Volkert - 2004 - The Austrian Grid Initiative – High Level Extensio.pdf:application/pdf},
}

@inproceedings{laforenza_next_2004,
	location = {Berlin, Heidelberg},
	title = {Next Generation Grid: Learn from the Past, Look to the Future},
	isbn = {978-3-540-30218-6},
	doi = {10.1007/978-3-540-30218-6_8},
	series = {Lecture Notes in Computer Science},
	shorttitle = {Next Generation Grid},
	abstract = {The first part of this talk will be focused on the Grid Evolution. In fact, in order to discuss “what is a Next Generation Grid”, it is important to determine “with respect to what”. Distinct phases in the evolution of Grids are observable. At the beginning of the 90’s, in order to tackle huge scientific problems, in several important research centers tests were conducted on the cooperative use of geographically distributed resources, conceived as a single powerful computer. In 1992, Charlie Catlett and Larry Smarr coined the term “Metacomputing” to describe this innovative computational approach [1].},
	pages = {11--12},
	booktitle = {Recent Advances in Parallel Virtual Machine and Message Passing Interface},
	publisher = {Springer},
	author = {Laforenza, Domenico},
	editor = {Kranzlmüller, Dieter and Kacsuk, Péter and Dongarra, Jack},
	date = {2004},
	langid = {english},
	file = {Full Text PDF:/Users/npvietkhoa/Zotero/storage/3ZRW9D73/Laforenza - 2004 - Next Generation Grid Learn from the Past, Look to.pdf:application/pdf},
}

@inproceedings{konya_advanced_2004,
	location = {Berlin, Heidelberg},
	title = {Advanced Resource Connector ({ARC}) – The Grid Middleware of the {NorduGrid}},
	isbn = {978-3-540-30218-6},
	doi = {10.1007/978-3-540-30218-6_7},
	series = {Lecture Notes in Computer Science},
	abstract = {The Advanced Resource Connector ({ARC}), or the {NorduGrid} middleware, is an open source software solution enabling production quality computational and data Grids. Since the first release (May 2002) the middleware is deployed and being used in production environments. Emphasis is put on scalability, stability, reliability and performance of the middleware. A growing number of grid deployments chose {ARC} as their middleware, thus building one of the largest production Grids of the world.},
	pages = {10--10},
	booktitle = {Recent Advances in Parallel Virtual Machine and Message Passing Interface},
	publisher = {Springer},
	author = {Kónya, Balázs},
	editor = {Kranzlmüller, Dieter and Kacsuk, Péter and Dongarra, Jack},
	date = {2004},
	langid = {english},
	file = {Full Text PDF:/Users/npvietkhoa/Zotero/storage/3IHDM7VS/Kónya - 2004 - Advanced Resource Connector (ARC) – The Grid Middl.pdf:application/pdf},
}

@inproceedings{dozsa_high_2004,
	location = {Berlin, Heidelberg},
	title = {High Performance Application Execution Scenarios in P-{GRADE}},
	isbn = {978-3-540-30218-6},
	doi = {10.1007/978-3-540-30218-6_5},
	series = {Lecture Notes in Computer Science},
	abstract = {The P-{GRADE} system provides high level graphical support for development and execution of high performance message-passing applications.Originally, execution of such applications was supported on heterogeneous workstation clusters in interactive mode only. Recently, the system has been substantially extended towards supporting job execution mode in Grid like execution environments. As a result, P-{GRADE} now makes possible to run the same {HPC} application using different message-passing middlewares ({PVM},{MPI} or {MPICH}-G2) in either interactive or job execution mode, on various execution resources controlled by either Condor or Globus-2 Grid middlewares.},
	pages = {8--8},
	booktitle = {Recent Advances in Parallel Virtual Machine and Message Passing Interface},
	publisher = {Springer},
	author = {Dózsa, Gábor},
	editor = {Kranzlmüller, Dieter and Kacsuk, Péter and Dongarra, Jack},
	date = {2004},
	langid = {english},
	file = {Full Text PDF:/Users/npvietkhoa/Zotero/storage/5CK653XJ/Dózsa - 2004 - High Performance Application Execution Scenarios i.pdf:application/pdf},
}

@article{widener_noise_2016,
	title = {On noise and the performance benefit of nonblocking collectives},
	volume = {30},
	issn = {1094-3420},
	url = {https://doi.org/10.1177/1094342015611952},
	doi = {10.1177/1094342015611952},
	abstract = {Relaxed synchronization offers the potential for maintaining application scalability, by allowing many processes to make independent progress when some processes suffer delays. Yet the benefits of this approach for important parallel workloads have not been investigated in detail. In this paper, we use a validated simulation approach to explore the noise-mitigation effects of idealized nonblocking collectives, in workloads where these collectives are a major contributor to total execution time. Although nonblocking collectives are unlikely to provide significant noise mitigation to applications in the low operating system noise environments expected in next-generation high-performance computing systems, we show that they can potentially improve application runtime with respect to other noise types.},
	pages = {121--133},
	number = {1},
	journaltitle = {The International Journal of High Performance Computing Applications},
	author = {Widener, Patrick M and Levy, Scott and Ferreira, Kurt B and Hoefler, Torsten},
	urldate = {2023-05-28},
	date = {2016-02-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Ltd {STM}},
	file = {SAGE PDF Full Text:/Users/npvietkhoa/Zotero/storage/D2TVP7CQ/Widener et al. - 2016 - On noise and the performance benefit of nonblockin.pdf:application/pdf},
}

@article{quinlan_rose_2023,
	title = {The {ROSE} Source-to-Source Compiler Infrastructure},
	abstract = {The development of future software for proposed new computer architectures is expected to make significant demands on compiler technologies. Significant rewriting of applications will be likely to support the use of new hardware features and preserve the current software investment. Existing compilers provide little support for the broad range of research efforts addressing exascale challenges, such as parallelism, locality, resiliency, power efficiency, etc. The economics of how new machines will change an existing code base of software, that is too expensive to manually rewrite, may well drive automated mechanisms to transform existing software to take advantage of future machine features. This approach will lessen the cost and delay of the move to new, and possibly radically different, future architectures. Source-to-source compilers provides a pragmatic vehicle to support research, development, and deployment of novel compiler technologies by compiler experts or even advanced application developers. Within a source-to-source approach the input source code is read by the compiler, an internal representation ({IR}) is constructed, the {IR} is the basis of analysis that is used to guide transformations, the transformations occur on the {IR}, the {IR} is used to regenerate new source code, which is then compiled by a backend compiler. Our source-to-source compiler, {ROSE}, is a project to support the requirements of {DOE}. Work on {ROSE} has focused on the development of a community based project to de-fine source-to-source compilation for a broad range of languages especially targeted at {DOE} applications (addressing robustness and large scale codes as required for {DOE} applications). Novel research areas are most easily supported when they can leverage significant tool chains that interact and use source code while allowing the hardware vendor's own compiler for low level optimizations. In fact, high level optimization are rarely feasible for existing low level compilers for common languages such as C, C++, and Fortran. {ROSE} addresses the economics of how compiler research can be moved closer to the audience with significant tech-nical performance problems and for whom the hardware is likely to be changing significantly in the next decade. Within {ROSE} it is less the goal to solve all problems than to permit domain experts to better solve their own problems. This talk will focus on the design and motivation for {ROSE} as an open community source-to-source compiler infrastructure to support performance optimization, tools for analysis, verification and software assurance, and general cus-[Copyright notice will appear here once 'preprint' option is removed.] tom analysis and transformations needs directly on software using the languages common within {DOE} High Performance Computing.},
	author = {Quinlan, Dan and Liao, Chunhua},
	date = {2023-05-28},
	file = {Full Text PDF:/Users/npvietkhoa/Zotero/storage/3T6K9SUB/Quinlan and Liao - 2023 - The ROSE Source-to-Source Compiler Infrastructure.pdf:application/pdf},
}

@book{baude_optimizing_2001,
	title = {Optimizing Metacomputing with Communication-Computation Overlap},
	volume = {2127},
	isbn = {978-3-540-42522-9},
	abstract = {In the framework of distributed object systems, this paper presents the concepts and an implementation of an overlapping mecha- nism between communication and computation. This mechanism allows to decrease the execution time of a remote method invocation with pa- rameters of large size. Its implementation and related experiments in the C++// language running on top of Globus and Nexus are described.},
	pagetotal = {190},
	author = {Baude, Françoise and Caromel, Denis and Furmento, Nathalie and Sagnol, David},
	date = {2001-09-03},
	doi = {10.1007/3-540-44743-1_19},
	note = {Pages: 204},
	file = {Full Text PDF:/Users/npvietkhoa/Zotero/storage/K8CAAB8X/Baude et al. - 2001 - Optimizing Metacomputing with Communication-Comput.pdf:application/pdf},
}

@article{calland_tiling_1999,
	title = {Tiling on systems with communication/computation overlap},
	volume = {11},
	rights = {Copyright © 1999 John Wiley \& Sons, Ltd.},
	issn = {1096-9128},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/%28SICI%291096-9128%28199903%2911%3A3%3C139%3A%3AAID-CPE370%3E3.0.CO%3B2-X},
	doi = {10.1002/(SICI)1096-9128(199903)11:3<139::AID-CPE370>3.0.CO;2-X},
	abstract = {In the framework of fully permutable loops, tiling is a compiler technique (also known as ‘loop blocking’) that has been extensively studied as a source-to-source program transformation. Little work has been devoted to the mapping and scheduling of the tiles on to physical parallel processors. We present several new results in the context of limited computational resources and assuming communication–computation overlap. In particular, under some reasonable assumptions, we derive the optimal mapping and scheduling of tiles to physical processors. Copyright © 1999 John Wiley \& Sons, Ltd.},
	pages = {139--153},
	number = {3},
	journaltitle = {Concurrency: Practice and Experience},
	author = {Calland, Pierre-Yves and Dongarra, Jack and Robert, Yves},
	urldate = {2023-05-30},
	date = {1999},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/\%28SICI\%291096-9128\%28199903\%2911\%3A3\%3C139\%3A\%3AAID-{CPE}370\%3E3.0.{CO}\%3B2-X},
	file = {Full Text PDF:/Users/npvietkhoa/Zotero/storage/WJMXQRHM/Calland et al. - 1999 - Tiling on systems with communicationcomputation o.pdf:application/pdf;Snapshot:/Users/npvietkhoa/Zotero/storage/BAGD2HBR/(SICI)1096-9128(199903)113139AID-CPE3703.0.html:text/html},
}

@book{danalis_transformations_2005,
	title = {Transformations to Parallel Codes for Communication-Computation Overlap},
	volume = {2005},
	isbn = {978-1-59593-061-3},
	abstract = {This paper presents program transformations directed toward improving communication-computation overlap in parallel programs that use {MPI}s collective operations. Our transformations target a wide variety of applications focusing on scientific codes with computation loops that exhibit limited dependence among iterations. We include guidance for developers for transforming an application code in order to exploit the communicationcomputation overlap available in the underlying cluster, as well as a discussion of the performance improvements achieved by our transformations. We present results from a detailed study of the effect of the problem and message size, level of communication-computation overlap, and amount of communication aggregation on runtime performance in a cluster environment based on an {RDMA}-enabled network. The targets of our study are two scientific codes written by domain scientists, but the applicability of our work extends far beyond the scope of these two applications.},
	pagetotal = {58},
	author = {Danalis, Anthony and Kim, Ki-Yong and Pollock, Lori and Swany, Martin},
	date = {2005-12-12},
	doi = {10.1109/SC.2005.75},
	note = {Journal Abbreviation: Proceedings of the {ACM}/{IEEE} 2005 Supercomputing Conference, {SC}'05
Pages: 58
Publication Title: Proceedings of the {ACM}/{IEEE} 2005 Supercomputing Conference, {SC}'05},
	file = {Full Text PDF:/Users/npvietkhoa/Zotero/storage/RA4DS828/Danalis et al. - 2005 - Transformations to Parallel Codes for Communicatio.pdf:application/pdf},
}

@article{quinlan_rose_2000,
	title = {{ROSE}: Compiler Support for Object-Oriented Frameworks.},
	volume = {10},
	doi = {10.1016/S0129-6264(00)00021-4},
	shorttitle = {{ROSE}},
	abstract = {{ROSE} is a preprocessor generation tool for the support of compile time performance optimizations of general object-oriented frameworks. Within this work {ROSE} is being applied first to Overture, a serial/parallel object-oriented framework for solving partial differential equations in two and three space dimensions. The optimization of the interactions between objects within Overture is of particular interest since the Overture applications can be computationally large (many millions of mesh points and iterating over thousands of time-steps). Unfortunately, optimizations that might be obvious to the framework developer or application developer (e.g. cache based optimizations), due to the precise semantics of the framework's abstractions, are often lost through the C++ compiler's inability to recognize or leverage such semantics. Preprocessing steps can be used to introduce transformations using the semantics of a framework's abstractions, but the development of such a preprocessor tool is particularly complicated for a general object-oriented language such as C++. This paper shows how such framework specific preprocessors can be automatically generated.
In this paper we briefly present Overture with some examples, and present our approach toward optimizing the performance for Overture and the A++P++ array class abstractions upon which Overture depends. The results we present show that the semantics of the abstractions represented within Overture and the A++P++ array class library can be used to generate a preprocessor using {ROSE}. The results demonstrate the performance of an Overture application with and without such a preprocessing step, the final performance with preprocessing is equivalent to that of optimized C and Fortran 77. By design, {ROSE} is general in its application to any object-oriented framework or application and is in no way specific to Overture.},
	pages = {215--226},
	journaltitle = {Parallel Processing Letters},
	shortjournal = {Parallel Processing Letters},
	author = {Quinlan, Daniel},
	date = {2000-06-01},
}

@book{laguna_large-scale_2019,
	title = {A large-scale study of {MPI} usage in open-source {HPC} applications},
	isbn = {978-1-4503-6229-0},
	abstract = {Understanding the state-of-the-practice in {MPI} usage is paramount for many aspects of supercomputing, including optimizing the communication of {HPC} applications and informing standardization bodies and {HPC} systems procurements regarding the most important {MPI} features. Unfortunately, no previous study has characterized the use of {MPI} on applications at a significant scale; previous surveys focus either on small data samples or on {MPI} jobs of specific {HPC} centers. This paper presents the first comprehensive study of {MPI} usage in applications. We survey more than one hundred distinct {MPI} programs covering a significantly large space of the population of {MPI} applications. We focus on understanding the characteristics of {MPI} usage with respect to the most used features, code complexity, and programming models and languages. Our study corroborates certain findings previously reported on smaller data samples and presents a number of interesting, previously un-reported insights.},
	author = {Laguna, Ignacio and Marshall, Ryan and Mohror, Kathryn and Ruefenacht, Martin and Skjellum, Anthony},
	date = {2019-11-17},
	doi = {10.1145/3295500.3356176},
	file = {Full Text PDF:/Users/npvietkhoa/Zotero/storage/E33BYBSN/Laguna et al. - 2019 - A large-scale study of MPI usage in open-source HP.pdf:application/pdf},
}

@inproceedings{pirkelbauer_source_2010,
	location = {Berlin, Heidelberg},
	title = {Source Code Rejuvenation Is Not Refactoring},
	isbn = {978-3-642-11266-9},
	doi = {10.1007/978-3-642-11266-9_53},
	series = {Lecture Notes in Computer Science},
	abstract = {Programmers rely on programming idioms, design patterns, and workaround techniques to make up for missing programming language support. Evolving languages often address frequently encountered problems by adding language and library support to subsequent releases. By using new features, programmers can express their intent more directly. As new concerns, such as parallelism or security, arise, early idioms and language facilities can become serious liabilities. Modern code sometimes benefits from optimization techniques not feasible for code that uses less expressive constructs. Manual source code migration is expensive, time-consuming, and prone to errors.},
	pages = {639--650},
	booktitle = {{SOFSEM} 2010: Theory and Practice of Computer Science},
	publisher = {Springer},
	author = {Pirkelbauer, Peter and Dechev, Damian and Stroustrup, Bjarne},
	editor = {van Leeuwen, Jan and Muscholl, Anca and Peleg, David and Pokorný, Jaroslav and Rumpe, Bernhard},
	date = {2010},
	langid = {english},
	keywords = {Code Transformation, Code Smell, Source Code, Standard Template Library, Template Function},
	file = {Full Text PDF:/Users/npvietkhoa/Zotero/storage/DXYBYEXM/Pirkelbauer et al. - 2010 - Source Code Rejuvenation Is Not Refactoring.pdf:application/pdf},
}

@article{courant_verified_2021,
	title = {Verified code generation for the polyhedral model},
	volume = {5},
	issn = {2475-1421},
	url = {https://dl.acm.org/doi/10.1145/3434321},
	doi = {10.1145/3434321},
	abstract = {The polyhedral model is a high-level intermediate representation for loop nests that supports elegantly a great many loop optimizations. In a compiler, after polyhedral loop optimizations have been performed, it is necessary and difficult to regenerate sequential or parallel loop nests before continuing compilation. This paper reports on the formalization and proof of semantic preservation of such a code generator that produces sequential code from a polyhedral representation. The formalization and proofs are mechanized using the Coq proof assistant.},
	pages = {1--24},
	issue = {{POPL}},
	journaltitle = {Proceedings of the {ACM} on Programming Languages},
	shortjournal = {Proc. {ACM} Program. Lang.},
	author = {Courant, Nathanaël and Leroy, Xavier},
	urldate = {2023-06-27},
	date = {2021-01-04},
	langid = {english},
	file = {Full Text PDF:/Users/npvietkhoa/Zotero/storage/YJQN6HF3/Courant and Leroy - 2021 - Verified code generation for the polyhedral model.pdf:application/pdf},
}

@incollection{margaria_verification_2014,
	location = {Berlin, Heidelberg},
	title = {Verification of Polyhedral Optimizations with Constant Loop Bounds in Finite State Space Computations},
	volume = {8803},
	isbn = {978-3-662-45230-1 978-3-662-45231-8},
	url = {http://link.springer.com/10.1007/978-3-662-45231-8_41},
	abstract = {As processors gain in complexity and heterogeneity, compilers are asked to perform program transformations of ever-increasing complexity to effectively map an input program to the target hardware. It is critical to develop methods and tools to automatically assert the correctness of programs generated by such modern optimizing compilers.},
	pages = {493--508},
	booktitle = {Leveraging Applications of Formal Methods, Verification and Validation. Specialized Techniques and Applications},
	publisher = {Springer Berlin Heidelberg},
	author = {Schordan, Markus and Lin, Pei-Hung and Quinlan, Dan and Pouchet, Louis-Noël},
	editor = {Margaria, Tiziana and Steffen, Bernhard},
	urldate = {2023-07-02},
	date = {2014},
	langid = {english},
	doi = {10.1007/978-3-662-45231-8_41},
	note = {Series Title: Lecture Notes in Computer Science},
	file = {Schordan et al. - 2014 - Verification of Polyhedral Optimizations with Cons.pdf:/Users/npvietkhoa/Zotero/storage/FTRCSIQP/Schordan et al. - 2014 - Verification of Polyhedral Optimizations with Cons.pdf:application/pdf},
}

@online{noauthor_nas_nodate,
	title = {{NAS} Parallel Benchmarks},
	url = {https://www.nas.nasa.gov/software/npb.html},
	urldate = {2023-07-03},
	file = {NAS Parallel Benchmarks:/Users/npvietkhoa/Zotero/storage/2DK4WMUY/npb.html:text/html},
}

@online{noauthor_pluto_nodate,
	title = {{PLUTO} - An automatic loop nest parallelizer and locality optimizer for multicores},
	url = {https://pluto-compiler.sourceforge.net/},
	urldate = {2023-07-03},
	file = {PLUTO - An automatic loop nest parallelizer and locality optimizer for multicores:/Users/npvietkhoa/Zotero/storage/P4ML3PYV/pluto-compiler.sourceforge.net.html:text/html},
}

@misc{frohn_adcl_2023,
	title = {{ADCL}: Acceleration Driven Clause Learning for Constrained Horn Clauses},
	url = {http://arxiv.org/abs/2303.01827},
	doi = {10.48550/arXiv.2303.01827},
	shorttitle = {{ADCL}},
	abstract = {Constrained Horn Clauses ({CHCs}) are often used in automated program verification. Thus, techniques for (dis-)proving satisfiability of {CHCs} are a very active field of research. On the other hand, acceleration techniques for computing formulas that characterize the N-fold closure of loops have successfully been used for static program analysis. We show how to use acceleration to avoid repeated derivations with recursive {CHCs} in resolution proofs, which reduces the length of the proofs drastically. This idea gives rise to a novel calculus for (dis)proving satisfiability of {CHCs}, called Acceleration Driven Clause Learning ({ADCL}). We implemented this new calculus in our tool {LoAT} and evaluate it empirically in comparison to other state-of-the-art tools.},
	number = {{arXiv}:2303.01827},
	publisher = {{arXiv}},
	author = {Frohn, Florian and Giesl, Jürgen},
	urldate = {2023-07-03},
	date = {2023-04-24},
	eprinttype = {arxiv},
	eprint = {2303.01827 [cs]},
	keywords = {Computer Science - Logic in Computer Science},
	file = {arXiv Fulltext PDF:/Users/npvietkhoa/Zotero/storage/VWYPAC8Y/Frohn and Giesl - 2023 - ADCL Acceleration Driven Clause Learning for Cons.pdf:application/pdf;arXiv.org Snapshot:/Users/npvietkhoa/Zotero/storage/CIRL8KFY/2303.html:text/html},
}

@report{lumsdaine_compiled_2013,
	title = {Compiled {MPI}: Cost-Effective Exascale Application Development},
	url = {http://www.osti.gov/servlets/purl/1107342/},
	shorttitle = {Compiled {MPI}},
	pages = {DOE--IU--0005323, 1107342},
	number = {{DOE}-{IU}-0005323, 1107342},
	author = {Lumsdaine, Andrew and Friedley, Andrew},
	urldate = {2023-07-03},
	date = {2013-09-25},
	langid = {english},
	doi = {10.2172/1107342},
	file = {Lumsdaine and Friedley - 2013 - Compiled MPI Cost-Effective Exascale Application .pdf:/Users/npvietkhoa/Zotero/storage/4XLE3ID9/Lumsdaine and Friedley - 2013 - Compiled MPI Cost-Effective Exascale Application .pdf:application/pdf},
}

@inproceedings{steensgaard_points-analysis_1996,
	location = {St. Petersburg Beach, Florida, United States},
	title = {Points-to analysis in almost linear time},
	isbn = {978-0-89791-769-8},
	url = {http://portal.acm.org/citation.cfm?doid=237721.237727},
	doi = {10.1145/237721.237727},
	abstract = {We present an interprocedural ﬂow-insensitive points-to analysis based on type inference methods with an almost linear time cost complexity. To our knowledge, this is the asymptotically fastest non-trivial interprocedural points-to analysis algorithm yet described. The algorithm is based on a non-standard type system. The type inferred for any variable represents a set of locations and includes a type which in turn represents a set of locations possibly pointed to by the variable. The type inferred for a function variable represents a set of functions it may point to and includes a type signature for these functions. The results are equivalent to those of a ﬂowinsensitive alias analysis (and control ﬂow analysis) that assumes alias relations are reﬂexive and transitive.},
	eventtitle = {the 23rd {ACM} {SIGPLAN}-{SIGACT} symposium},
	pages = {32--41},
	booktitle = {Proceedings of the 23rd {ACM} {SIGPLAN}-{SIGACT} symposium on Principles of programming languages  - {POPL} '96},
	publisher = {{ACM} Press},
	author = {Steensgaard, Bjarne},
	urldate = {2023-07-03},
	date = {1996},
	langid = {english},
	file = {Steensgaard - 1996 - Points-to analysis in almost linear time.pdf:/Users/npvietkhoa/Zotero/storage/8XJYDUJY/Steensgaard - 1996 - Points-to analysis in almost linear time.pdf:application/pdf},
}

@manual{noauthor_mpi_nodate,
    author = "{Message Passing Interface Forum}",
    title  = "{MPI}: A Message-Passing Interface Standard Version 4.0",
    url    = "https://www.mpi-forum.org/docs/mpi-4.0/mpi40-report.pdf",
    year   = 2021,
    month  = jun
}

@misc{Tullos_2014, title={Improving Performance with MPI-3 Non-Blocking Collectives}, url={https://software.intel.com/content/www/us/en/develop/articles/improving-performance-with-mpi-3-non-blocking-collectives.html}, journal={mproving Performance with MPI-3 Non-Blocking Collectives}, publisher={James Tullos }, author={Tullos, James}, year={2014}, month={Jan}} 