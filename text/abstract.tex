\noindent
In parallel applications, even when there is ample parallelism and effective load balancing, a common challenge arises that negatively impacts performance. Processes idling while waiting for remote data from other processes leads to a reduced efficiency of the program. Network latencies and synchronization among processes are two significant factors which can lead to these communication delays. To address this problem, one potential approach is to reduce the influence of communication costs by overlapping them with local computational tasks, effectively concealing the additional time and resources required for communication.
While manual transformation is possible, it is often complex and prone to errors. To address this issue, several frameworks and tools have been developed to automate the transformation process. This paper aims to gather the key contributions related to the overlapping of computation and communication in MPI communications. Additionally, it provides a technical explanation of how such communications can be automatically transformed using specific frameworks and tools.
