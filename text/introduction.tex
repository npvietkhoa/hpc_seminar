\section{Introduction}
MPI \cite{noauthor_mpi_nodate} library is usually used for parallel high-performance computing that includes a wide variety of point-to-point and group communication operations. MPI enables efficient communication by eliminating the need for memory-to-memory copying, enabling computation and communication to occur simultaneously, and facilitating offloading to communication coprocessors, if they are present. 
In distributed computing scenarios, communication plays a crucially significant role, encompassing collaborative multimedia applications, parallel computing across workstation clusters, and high-performance computing over computational grids. By effectively optimizing communication mechanisms, these systems can derive substantial benefits and achieve enhanced performance levels. 

Enhancing basic communication performance can be accomplished through the utilization of advanced communication hardware and specialized middleware. This approach aims to reduce latency and enhance bandwidth. However, focusing solely on bandwidth and latency has its limitations when it comes to improving overall application performance. To overcome these limitations, strategies that integrate computation and communication have been developed. By simultaneously performing communication and computation, these approaches aim to hide the costs associated with latency. Achieving true overlap and reaping the associated performance benefits requires support from both hardware and middleware, which can sometimes place significant demands on communication hardware. To harness this parallelism effectively, programmers must have access to specific non-blocking semantics. \cite{hoefler_leveraging_2008}\\
For example, for applications using collective operations, a simple textual replacement of blocking to non-blocking communication does not suffice to derive any significant benefit. To attain advantages of non-blocking features, developers have to identify independent parts in the code that can be interposed between a collective call and WAIT/ TEST (compatibly across the group of the MPI communicator) in order to expose computation that might overlap with the non-blocking collective.\\
Previous work \cite{calland_tiling_1999}\cite{danalis_transformations_2005}\cite{baude_optimizing_2001} presents  the techniques and schemas to manually transform blocking to non-blocking communication. Torsten Hoefler \cite{hoefler_leveraging_2008} et al suggest possibilities to leverage performance for manual transformation. It requires some techniques such as loop tiling, loop indexing and loop distribution and tuning parameters which need to be done manually and require deep understanding about the system in order to maximize overlapping to achieve better overlapping. Transforming applications manually in such way requires a lot of effort to tune the parameters and is potentially error-prone and time-consuming.\cite{sancho_mpi_2006}

In order to simplify the developer's work and to help ensure periodic introduction of new library features seamlessly into these applications, the goal of this work is to automate the process of changing operations from blocking to non-blocking as well as to the expected corresponding near-future persistent variants thereof.

In this paper, I present compiler-based tools and frameworks aimed at automating the transformation of user programs that rely on blocking operations into more efficient non-blocking variants. By automatically implementing these transformations, these transformations, the goal is to achieve enhanced performance in parallel computing scenarios.

In Section 2, we provide a comprehensive overview of standard MPI communications, contextualizing the need for automatic transformations for our proposed transformations. Additionally, we briefly outline the key principles underlying the transformation approach. Building upon this foundation, Section 3 focuses on exploring in detail and the frameworks and Petal tools employed, showcasing their advanced capabilities in automatically transforming the code.
To demonstrate the effectiveness of our these tools, Section 4 presents a specific case study  where the presented transformations were successfully applied to a parallel code. By effectively overlapping communication and computation, this transformed code achieved performance gains.

Subsequent sections delve into important areas of discussion, including the identification of potential avenues for future research, an examination of the limitations inherent in the presented tools and frameworks, and finally, the presentation of our conclusive findings.
